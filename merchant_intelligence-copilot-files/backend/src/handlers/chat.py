"""
Chat Handler - LLM-powered conversational interface
Uses Bedrock Nova for intelligent responses based on insights data
"""

import json
import logging
from typing import Dict, Any
from common.responses import ok, bad
from common.config import BEDROCK_MODEL_FAST, BEDROCK_MODEL_BASELINE, AWS_REGION
from common.bedrock_nova import nova_converse
from common.validators import validate_prompt_injection

logger = logging.getLogger()
logger.setLevel(logging.INFO)


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Handle chat requests with LLM-powered responses.
    
    Expected request body:
    {
        "message": "Which products should I order?",
        "language": "en",  # Optional: "en", "hi", "mr"
        "insights": {...}  # Optional: current insights data for context
    }
    """
    try:
        # Parse request body
        body = json.loads(event.get('body', '{}'))
        message = body.get('message', '').strip()
        language = body.get('language', 'en')
        insights = body.get('insights')
        
        if not message:
            return bad("Message is required")
        
        # Validate for prompt injection
        if not validate_prompt_injection(message):
            return bad("Invalid message content")
        
        logger.info(f"Chat request - Message: {message[:100]}, Language: {language}")
        
        # Log insights data structure for debugging
        if insights:
            if 'insights' in insights and isinstance(insights['insights'], dict):
                products_count = len(insights['insights'].get('products', []))
                logger.info(f"Insights structure: nested, products count: {products_count}")
            elif 'products' in insights:
                products_count = len(insights.get('products', []))
                logger.info(f"Insights structure: flat, products count: {products_count}")
            else:
                logger.warning(f"Insights structure unknown: {list(insights.keys())}")
        else:
            logger.info("No insights data provided")
        
        # Generate LLM response with context
        response_text = generate_llm_response(message, language, insights)
        
        return ok({
            'response': response_text,
            'language': language,
            'disclaimer': 'AI-assisted insights. Review with your business knowledge.'
        })
        
    except Exception as e:
        logger.error(f"Error in chat handler: {str(e)}", exc_info=True)
        return bad(f"Error processing chat request: {str(e)}")


def generate_llm_response(message: str, language: str, insights: Dict = None) -> str:
    """
    Generate fast, context-aware responses with rule-based logic for common queries.
    Only use LLM for complex questions.
    """
    import time
    start_time = time.time()
    
    message_lower = message.lower()
    logger.info(f"Processing message: '{message_lower}'")
    
    # Check if insights data is available and extract products
    products = []
    if insights:
        # Handle nested structure: insights.insights.products or insights.products
        if 'insights' in insights and isinstance(insights['insights'], dict):
            products = insights['insights'].get('products', [])
        elif 'products' in insights:
            products = insights.get('products', [])
    
    if not products:
        logger.info(f"No products found, returning no data response (took {time.time() - start_time:.3f}s)")
        return get_no_data_response(language)
    
    logger.info(f"Found {len(products)} products")
    
    high_urgency = [p for p in products if p.get('reorder', {}).get('urgency') == 'high']
    medium_urgency = [p for p in products if p.get('reorder', {}).get('urgency') == 'medium']
    anomalies = [p for p in products if p.get('anomalies') and len(p['anomalies']) > 0]
    low_confidence = [p for p in products if p.get('confidence_score', 100) < 60]
    
    # Fast rule-based responses for common questions with expanded keywords
    
    # Question: Which products to order / reorder
    order_keywords = ['order', 'reorder', 'stock', 'buy', 'purchase', 'should i', 'which product', 'what product', 'need to order', 'this week', 'рдорд╛рдЧрд╡рд╛рд╡реА', 'рдСрд░реНрдбрд░', 'рдХреМрди рд╕реЗ рдЙрддреНрдкрд╛рдж', 'рдХреЛрдгрддреА рдЙрддреНрдкрд╛рджрдиреЗ']
    if any(word in message_lower for word in order_keywords):
        logger.info(f"Matched ORDER keywords (took {time.time() - start_time:.3f}s)")
        return generate_reorder_response(high_urgency, medium_urgency, language)
    
    # Question: Top selling products
    top_keywords = ['top', 'best', 'selling', 'popular', 'most', 'highest', 'leading', 'what are my', 'рд╕рдмрд╕реЗ', 'рд╕рд░реНрд╡рд╛рдзрд┐рдХ', 'рдмрд┐рдХрдиреЗ рд╡рд╛рд▓реЗ', 'рд╡рд┐рдХреНрд░реА рд╣реЛрдгрд╛рд░реА']
    if any(word in message_lower for word in top_keywords):
        logger.info(f"Matched TOP keywords (took {time.time() - start_time:.3f}s)")
        return generate_top_products_response(products, language)
    
    # Question: Alerts / anomalies / spikes
    alert_keywords = ['alert', 'anomaly', 'spike', 'unusual', 'strange', 'drop', 'demand', 'are there any', 'рдЕрд▓рд░реНрдЯ', 'рд╡реГрджреНрдзрд┐', 'рдорд╛рдВрдЧ', 'рдХрд╛рд╣реА', 'рдЖрд╣реЗ рдХрд╛']
    if any(word in message_lower for word in alert_keywords):
        logger.info(f"Matched ALERT keywords (took {time.time() - start_time:.3f}s)")
        return generate_alerts_response(anomalies, language)
    
    # Question: Forecast / prediction
    forecast_keywords = ['forecast', 'predict', 'future', 'next', 'expect', 'ahead', 'coming', 'рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди', 'рдЕрдВрджрд╛рдЬ']
    if any(word in message_lower for word in forecast_keywords):
        logger.info(f"Matched FORECAST keywords (took {time.time() - start_time:.3f}s)")
        return generate_forecast_response(products, language)
    
    # Question: Low confidence / data quality
    confidence_keywords = ['confidence', 'accuracy', 'reliable', 'trust', 'quality', 'рд╡рд┐рд╢реНрд╡рд╛рд╕', 'рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕']
    if any(word in message_lower for word in confidence_keywords):
        logger.info(f"Matched CONFIDENCE keywords (took {time.time() - start_time:.3f}s)")
        return generate_confidence_response(low_confidence, products, language)
    
    # For complex questions, use LLM with optimized context
    logger.info(f"No keyword match, using LLM (took {time.time() - start_time:.3f}s so far)")
    return generate_llm_complex_response(message, language, products, high_urgency, anomalies)


def generate_reorder_response(high_urgency: list, medium_urgency: list, language: str) -> str:
    """Fast response for reorder questions"""
    if language == 'en':
        if not high_urgency and not medium_urgency:
            return "тЬЕ Good news! All products have sufficient stock levels. No urgent reorders needed right now."
        
        response = "ЁЯУж Reorder Recommendations:\n\n"
        if high_urgency:
            response += "ЁЯФ┤ HIGH PRIORITY (Order immediately):\n"
            for p in high_urgency[:5]:
                response += f"тАв {p['product_name']} - Order {p['reorder']['quantity']} units\n"
        if medium_urgency:
            response += f"\nЁЯЯб MEDIUM PRIORITY (Order within 3-5 days):\n"
            for p in medium_urgency[:3]:
                response += f"тАв {p['product_name']} - Order {p['reorder']['quantity']} units\n"
        return response
    
    elif language == 'hi':
        if not high_urgency and not medium_urgency:
            return "тЬЕ рдЕрдЪреНрдЫреА рдЦрдмрд░! рд╕рднреА рдЙрддреНрдкрд╛рджреЛрдВ рдореЗрдВ рдкрд░реНрдпрд╛рдкреНрдд рд╕реНрдЯреЙрдХ рд╣реИред рдЕрднреА рдХреЛрдИ рддрддреНрдХрд╛рд▓ рдкреБрдирдГ рдСрд░реНрдбрд░ рдХреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдирд╣реАрдВред"
        
        response = "ЁЯУж рдкреБрдирдГ рдСрд░реНрдбрд░ рд╕рд┐рдлрд╛рд░рд┐рд╢реЗрдВ:\n\n"
        if high_urgency:
            response += "ЁЯФ┤ рдЙрдЪреНрдЪ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ (рддреБрд░рдВрдд рдСрд░реНрдбрд░ рдХрд░реЗрдВ):\n"
            for p in high_urgency[:5]:
                response += f"тАв {p['product_name']} - {p['reorder']['quantity']} рдпреВрдирд┐рдЯ рдСрд░реНрдбрд░ рдХрд░реЗрдВ\n"
        if medium_urgency:
            response += f"\nЁЯЯб рдордзреНрдпрдо рдкреНрд░рд╛рдердорд┐рдХрддрд╛ (3-5 рджрд┐рдиреЛрдВ рдореЗрдВ рдСрд░реНрдбрд░ рдХрд░реЗрдВ):\n"
            for p in medium_urgency[:3]:
                response += f"тАв {p['product_name']} - {p['reorder']['quantity']} рдпреВрдирд┐рдЯ рдСрд░реНрдбрд░ рдХрд░реЗрдВ\n"
        return response
    
    else:  # Marathi
        if not high_urgency and not medium_urgency:
            return "тЬЕ рдЪрд╛рдВрдЧрд▓реА рдмрд╛рддрдореА! рд╕рд░реНрд╡ рдЙрддреНрдкрд╛рджрдирд╛рдВрдордзреНрдпреЗ рдкреБрд░реЗрд╕рд╛ рд╕реНрдЯреЙрдХ рдЖрд╣реЗред рдЖрддреНрддрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рддрд╛рддрдбреАрдЪреНрдпрд╛ рдкреБрдиреНрд╣рд╛ рдСрд░реНрдбрд░рдЪреА рдЧрд░рдЬ рдирд╛рд╣реА."
        
        response = "ЁЯУж рдкреБрдиреНрд╣рд╛ рдСрд░реНрдбрд░ рд╢рд┐рдлрд╛рд░рд╕реА:\n\n"
        if high_urgency:
            response += "ЁЯФ┤ рдЙрдЪреНрдЪ рдкреНрд░рд╛рдзрд╛рдиреНрдп (рд▓рдЧреЗрдЪ рдСрд░реНрдбрд░ рдХрд░рд╛):\n"
            for p in high_urgency[:5]:
                response += f"тАв {p['product_name']} - {p['reorder']['quantity']} рдпреБрдирд┐рдЯ рдСрд░реНрдбрд░ рдХрд░рд╛\n"
        if medium_urgency:
            response += f"\nЁЯЯб рдордзреНрдпрдо рдкреНрд░рд╛рдзрд╛рдиреНрдп (3-5 рджрд┐рд╡рд╕рд╛рдВрдд рдСрд░реНрдбрд░ рдХрд░рд╛):\n"
            for p in medium_urgency[:3]:
                response += f"тАв {p['product_name']} - {p['reorder']['quantity']} рдпреБрдирд┐рдЯ рдСрд░реНрдбрд░ рдХрд░рд╛\n"
        return response


def generate_top_products_response(products: list, language: str) -> str:
    """Fast response for top products questions"""
    sorted_products = sorted(products, key=lambda p: sum([f.get('yhat', 0) for f in p.get('forecast', [])]), reverse=True)[:5]
    
    if language == 'en':
        response = "ЁЯФЭ Top Selling Products:\n\n"
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} units (7-day forecast)\n"
        response += "\nЁЯТб Tip: Keep higher stock levels for these products to avoid stockouts."
        return response
    
    elif language == 'hi':
        response = "ЁЯФЭ рд╕рдмрд╕реЗ рдЬрд╝реНрдпрд╛рджрд╛ рдмрд┐рдХрдиреЗ рд╡рд╛рд▓реЗ рдЙрддреНрдкрд╛рдж:\n\n"
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} рдпреВрдирд┐рдЯ (7-рджрд┐рди рдХрд╛ рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди)\n"
        response += "\nЁЯТб рд╕реБрдЭрд╛рд╡: рд╕реНрдЯреЙрдХрдЖрдЙрдЯ рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдП рдЗрди рдЙрддреНрдкрд╛рджреЛрдВ рдХрд╛ рдЕрдзрд┐рдХ рд╕реНрдЯреЙрдХ рд░рдЦреЗрдВред"
        return response
    
    else:  # Marathi
        response = "ЁЯФЭ рд╕рд░реНрд╡рд╛рдзрд┐рдХ рд╡рд┐рдХреНрд░реА рд╣реЛрдгрд╛рд░реА рдЙрддреНрдкрд╛рджрдиреЗ:\n\n"
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} рдпреБрдирд┐рдЯ (7-рджрд┐рд╡рд╕рд╛рдВрдЪрд╛ рдЕрдВрджрд╛рдЬ)\n"
        response += "\nЁЯТб рдЯреАрдк: рд╕реНрдЯреЙрдХрдЖрдЙрдЯ рдЯрд╛рд│рдгреНрдпрд╛рд╕рд╛рдареА рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдВрдЪрд╛ рдЬрд╛рд╕реНрдд рд╕реНрдЯреЙрдХ рдареЗрд╡рд╛."
        return response


def generate_alerts_response(anomalies: list, language: str) -> str:
    """Fast response for alerts/anomalies questions"""
    if language == 'en':
        if not anomalies:
            return "тЬЕ No unusual patterns detected. All products showing normal demand trends."
        
        response = "тЪая╕П Demand Alerts:\n\n"
        for p in anomalies[:5]:
            response += f"тАв {p['product_name']}:\n"
            for anomaly in p['anomalies'][:2]:
                response += f"  - {anomaly}\n"
        response += "\nЁЯТб Review these products and adjust inventory/pricing accordingly."
        return response
    
    elif language == 'hi':
        if not anomalies:
            return "тЬЕ рдХреЛрдИ рдЕрд╕рд╛рдорд╛рдиреНрдп рдкреИрдЯрд░реНрди рдирд╣реАрдВ рдорд┐рд▓рд╛ред рд╕рднреА рдЙрддреНрдкрд╛рдж рд╕рд╛рдорд╛рдиреНрдп рдорд╛рдВрдЧ рд░реБрдЭрд╛рди рджрд┐рдЦрд╛ рд░рд╣реЗ рд╣реИрдВред"
        
        response = "тЪая╕П рдорд╛рдВрдЧ рдЕрд▓рд░реНрдЯ:\n\n"
        for p in anomalies[:5]:
            response += f"тАв {p['product_name']}:\n"
            for anomaly in p['anomalies'][:2]:
                response += f"  - {anomaly}\n"
        response += "\nЁЯТб рдЗрди рдЙрддреНрдкрд╛рджреЛрдВ рдХреА рд╕рдореАрдХреНрд╖рд╛ рдХрд░реЗрдВ рдФрд░ рддрджрдиреБрд╕рд╛рд░ рдЗрдиреНрд╡реЗрдВрдЯрд░реА/рдореВрд▓реНрдп рдирд┐рд░реНрдзрд╛рд░рдг рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░реЗрдВред"
        return response
    
    else:  # Marathi
        if not anomalies:
            return "тЬЕ рдХреЛрдгрддреЗрд╣реА рдЕрд╕рд╛рдорд╛рдиреНрдп рдкреЕрдЯрд░реНрди рдЖрдврд│рд▓реЗ рдирд╛рд╣реАрдд. рд╕рд░реНрд╡ рдЙрддреНрдкрд╛рджрдиреЗ рд╕рд╛рдорд╛рдиреНрдп рдорд╛рдЧрдгреА рдЯреНрд░реЗрдВрдб рджрд░реНрд╢рд╡рдд рдЖрд╣реЗрдд."
        
        response = "тЪая╕П рдорд╛рдЧрдгреА рдЕрд▓рд░реНрдЯ:\n\n"
        for p in anomalies[:5]:
            response += f"тАв {p['product_name']}:\n"
            for anomaly in p['anomalies'][:2]:
                response += f"  - {anomaly}\n"
        response += "\nЁЯТб рдпрд╛ рдЙрддреНрдкрд╛рджрдирд╛рдВрдЪреЗ рдкреБрдирд░рд╛рд╡рд▓реЛрдХрди рдХрд░рд╛ рдЖрдгрд┐ рддреНрдпрд╛рдиреБрд╕рд╛рд░ рдЗрдиреНрд╡реНрд╣реЗрдВрдЯрд░реА/рдХрд┐рдВрдордд рд╕рдорд╛рдпреЛрдЬрд┐рдд рдХрд░рд╛."
        return response


def generate_forecast_response(products: list, language: str) -> str:
    """Fast response for forecast questions"""
    total_forecast = sum([sum([f.get('yhat', 0) for f in p.get('forecast', [])]) for p in products])
    avg_confidence = sum([p.get('confidence_score', 0) for p in products]) / len(products) if products else 0
    
    if language == 'en':
        response = f"ЁЯУК 7-Day Forecast Summary:\n\n"
        response += f"тАв Total expected sales: {total_forecast:.0f} units\n"
        response += f"тАв Average confidence: {avg_confidence:.0f}%\n"
        response += f"тАв Products analyzed: {len(products)}\n\n"
        response += "Top 3 products by forecast:\n"
        sorted_products = sorted(products, key=lambda p: sum([f.get('yhat', 0) for f in p.get('forecast', [])]), reverse=True)[:3]
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} units\n"
        return response
    
    elif language == 'hi':
        response = f"ЁЯУК 7-рджрд┐рди рдХрд╛ рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рд╕рд╛рд░рд╛рдВрд╢:\n\n"
        response += f"тАв рдХреБрд▓ рдЕрдкреЗрдХреНрд╖рд┐рдд рдмрд┐рдХреНрд░реА: {total_forecast:.0f} рдпреВрдирд┐рдЯ\n"
        response += f"тАв рдФрд╕рдд рд╡рд┐рд╢реНрд╡рд╛рд╕: {avg_confidence:.0f}%\n"
        response += f"тАв рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдЙрддреНрдкрд╛рдж: {len(products)}\n\n"
        response += "рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рдХреЗ рдЕрдиреБрд╕рд╛рд░ рд╢реАрд░реНрд╖ 3 рдЙрддреНрдкрд╛рдж:\n"
        sorted_products = sorted(products, key=lambda p: sum([f.get('yhat', 0) for f in p.get('forecast', [])]), reverse=True)[:3]
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} рдпреВрдирд┐рдЯ\n"
        return response
    
    else:  # Marathi
        response = f"ЁЯУК 7-рджрд┐рд╡рд╕рд╛рдВрдЪрд╛ рдЕрдВрджрд╛рдЬ рд╕рд╛рд░рд╛рдВрд╢:\n\n"
        response += f"тАв рдПрдХреВрдг рдЕрдкреЗрдХреНрд╖рд┐рдд рд╡рд┐рдХреНрд░реА: {total_forecast:.0f} рдпреБрдирд┐рдЯ\n"
        response += f"тАв рд╕рд░рд╛рд╕рд░реА рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕: {avg_confidence:.0f}%\n"
        response += f"тАв рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдЙрддреНрдкрд╛рджрдиреЗ: {len(products)}\n\n"
        response += "рдЕрдВрджрд╛рдЬрд╛рдиреБрд╕рд╛рд░ рд╢реАрд░реНрд╖ 3 рдЙрддреНрдкрд╛рджрдиреЗ:\n"
        sorted_products = sorted(products, key=lambda p: sum([f.get('yhat', 0) for f in p.get('forecast', [])]), reverse=True)[:3]
        for i, p in enumerate(sorted_products, 1):
            forecast_sum = sum([f.get('yhat', 0) for f in p.get('forecast', [])])
            response += f"{i}. {p['product_name']} - {forecast_sum:.0f} рдпреБрдирд┐рдЯ\n"
        return response


def generate_confidence_response(low_confidence: list, all_products: list, language: str) -> str:
    """Fast response for confidence/accuracy questions"""
    avg_confidence = sum([p.get('confidence_score', 0) for p in all_products]) / len(all_products) if all_products else 0
    
    if language == 'en':
        response = f"ЁЯУИ Forecast Confidence Report:\n\n"
        response += f"тАв Average confidence: {avg_confidence:.0f}%\n"
        response += f"тАв Products analyzed: {len(all_products)}\n"
        response += f"тАв Low confidence items: {len(low_confidence)}\n\n"
        
        if low_confidence:
            response += "тЪая╕П Products with low confidence (<60%):\n"
            for p in low_confidence[:3]:
                response += f"тАв {p['product_name']} - {p['confidence_score']:.0f}%\n"
            response += "\nЁЯТб Low confidence may indicate insufficient data or irregular patterns."
        else:
            response += "тЬЕ All forecasts have good confidence levels!"
        return response
    
    elif language == 'hi':
        response = f"ЁЯУИ рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рд╡рд┐рд╢реНрд╡рд╛рд╕ рд░рд┐рдкреЛрд░реНрдЯ:\n\n"
        response += f"тАв рдФрд╕рдд рд╡рд┐рд╢реНрд╡рд╛рд╕: {avg_confidence:.0f}%\n"
        response += f"тАв рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдЙрддреНрдкрд╛рдж: {len(all_products)}\n"
        response += f"тАв рдХрдо рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╡рд╛рд▓реЗ рдЖрдЗрдЯрдо: {len(low_confidence)}\n\n"
        
        if low_confidence:
            response += "тЪая╕П рдХрдо рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╡рд╛рд▓реЗ рдЙрддреНрдкрд╛рдж (<60%):\n"
            for p in low_confidence[:3]:
                response += f"тАв {p['product_name']} - {p['confidence_score']:.0f}%\n"
            response += "\nЁЯТб рдХрдо рд╡рд┐рд╢реНрд╡рд╛рд╕ рдЕрдкрд░реНрдпрд╛рдкреНрдд рдбреЗрдЯрд╛ рдпрд╛ рдЕрдирд┐рдпрдорд┐рдд рдкреИрдЯрд░реНрди рдХрд╛ рд╕рдВрдХреЗрдд рд╣реЛ рд╕рдХрддрд╛ рд╣реИред"
        else:
            response += "тЬЕ рд╕рднреА рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рдиреЛрдВ рдореЗрдВ рдЕрдЪреНрдЫрд╛ рд╡рд┐рд╢реНрд╡рд╛рд╕ рд╕реНрддрд░ рд╣реИ!"
        return response
    
    else:  # Marathi
        response = f"ЁЯУИ рдЕрдВрджрд╛рдЬ рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдЕрд╣рд╡рд╛рд▓:\n\n"
        response += f"тАв рд╕рд░рд╛рд╕рд░реА рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕: {avg_confidence:.0f}%\n"
        response += f"тАв рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдЙрддреНрдкрд╛рджрдиреЗ: {len(all_products)}\n"
        response += f"тАв рдХрдореА рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдЕрд╕рд▓реЗрд▓реЗ рдЖрдпрдЯрдо: {len(low_confidence)}\n\n"
        
        if low_confidence:
            response += "тЪая╕П рдХрдореА рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдЕрд╕рд▓реЗрд▓реА рдЙрддреНрдкрд╛рджрдиреЗ (<60%):\n"
            for p in low_confidence[:3]:
                response += f"тАв {p['product_name']} - {p['confidence_score']:.0f}%\n"
            response += "\nЁЯТб рдХрдореА рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдЕрдкреБрд░рд╛ рдбреЗрдЯрд╛ рдХрд┐рдВрд╡рд╛ рдЕрдирд┐рдпрдорд┐рдд рдкреЕрдЯрд░реНрдирдЪреЗ рд╕рдВрдХреЗрдд рдЕрд╕реВ рд╢рдХрддреЗ."
        else:
            response += "тЬЕ рд╕рд░реНрд╡ рдЕрдВрджрд╛рдЬрд╛рдВрдордзреНрдпреЗ рдЪрд╛рдВрдЧрд▓рд╛ рдЖрддреНрдорд╡рд┐рд╢реНрд╡рд╛рд╕ рдкрд╛рддрд│реА рдЖрд╣реЗ!"
        return response


def generate_llm_complex_response(message: str, language: str, products: list, high_urgency: list, anomalies: list) -> str:
    """Use fastest LLM (Nova Micro) for complex questions"""
    from common.config import BEDROCK_MODEL_BASELINE  # Fastest model
    
    logger.info("Complex question detected, using Nova Micro for fast response")
    
    lang_instruction = {
        'en': 'Respond in English',
        'hi': 'Respond in Hindi (рд╣рд┐рдВрджреА)',
        'mr': 'Respond in Marathi (рдорд░рд╛рдареА)'
    }.get(language, 'Respond in English')
    
    # Minimal context for fastest LLM response
    context = f"Products: {len(products)}, High urgency: {len(high_urgency)}, Anomalies: {len(anomalies)}"
    
    system = f"""You are a business advisor for Indian MSME merchants. {lang_instruction}.
Be very concise (2-3 sentences max), actionable, and use simple language."""
    
    user = f"{context}\n\nQuestion: {message}\n\nProvide a brief answer."
    
    try:
        response = nova_converse(BEDROCK_MODEL_BASELINE, system, user)  # Using fastest model
        return response
    except Exception as e:
        error_msg = str(e)
        logger.error(f"LLM generation failed: {error_msg}")
        
        # Return error message for debugging
        if "access denied" in error_msg.lower() or "AccessDeniedException" in error_msg:
            return f"тЪая╕П AWS Bedrock Access Issue: {error_msg}\n\nPlease enable Bedrock in your AWS account and request access to Nova models in the {AWS_REGION} region."
        elif "not found" in error_msg.lower() or "ResourceNotFoundException" in error_msg:
            return f"тЪая╕П Model Not Available: {error_msg}\n\nThe Nova model may not be available in your region or account."
        elif "credentials" in error_msg.lower():
            return f"тЪая╕П AWS Credentials Issue: {error_msg}\n\nPlease check your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY."
        
        # Fallback to helpful menu
        if language == 'en':
            return f"""I can help you with:

ЁЯУж Reorder recommendations - Ask "Which products should I order?"
ЁЯФЭ Top products - Ask "What are my top selling products?"
тЪая╕П Alerts - Ask "Are there any demand spikes?"
ЁЯУК Forecasts - Ask "What's the forecast for next week?"

You have {len(products)} products analyzed with {len(high_urgency)} high priority reorders and {len(anomalies)} alerts."""
        
        elif language == 'hi':
            return f"""рдореИрдВ рдЖрдкрдХреА рдорджрдж рдХрд░ рд╕рдХрддрд╛ рд╣реВрдВ:

ЁЯУж рдкреБрдирдГ рдСрд░реНрдбрд░ рд╕рд┐рдлрд╛рд░рд┐рд╢реЗрдВ - рдкреВрдЫреЗрдВ "рдореБрдЭреЗ рдХреМрди рд╕реЗ рдЙрддреНрдкрд╛рдж рдСрд░реНрдбрд░ рдХрд░рдиреЗ рдЪрд╛рд╣рд┐рдП?"
ЁЯФЭ рд╢реАрд░реНрд╖ рдЙрддреНрдкрд╛рдж - рдкреВрдЫреЗрдВ "рдореЗрд░реЗ рд╕рдмрд╕реЗ рдЬрд╝реНрдпрд╛рджрд╛ рдмрд┐рдХрдиреЗ рд╡рд╛рд▓реЗ рдЙрддреНрдкрд╛рдж рдХреМрди рд╕реЗ рд╣реИрдВ?"
тЪая╕П рдЕрд▓рд░реНрдЯ - рдкреВрдЫреЗрдВ "рдХреНрдпрд╛ рдХреЛрдИ рдорд╛рдВрдЧ рдореЗрдВ рд╡реГрджреНрдзрд┐ рд╣реИ?"
ЁЯУК рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди - рдкреВрдЫреЗрдВ "рдЕрдЧрд▓реЗ рд╕рдкреНрддрд╛рд╣ рдХрд╛ рдкреВрд░реНрд╡рд╛рдиреБрдорд╛рди рдХреНрдпрд╛ рд╣реИ?"

рдЖрдкрдХреЗ рдкрд╛рд╕ {len(products)} рдЙрддреНрдкрд╛рдж рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рд╣реИрдВ рдЬрд┐рдирдореЗрдВ {len(high_urgency)} рдЙрдЪреНрдЪ рдкреНрд░рд╛рдердорд┐рдХрддрд╛ рдкреБрдирдГ рдСрд░реНрдбрд░ рдФрд░ {len(anomalies)} рдЕрд▓рд░реНрдЯ рд╣реИрдВред"""
        
        else:  # Marathi
            return f"""рдореА рддреБрдореНрд╣рд╛рд▓рд╛ рдорджрдд рдХрд░реВ рд╢рдХрддреЛ:

ЁЯУж рдкреБрдиреНрд╣рд╛ рдСрд░реНрдбрд░ рд╢рд┐рдлрд╛рд░рд╕реА - рд╡рд┐рдЪрд╛рд░рд╛ "рдорд▓рд╛ рдХреЛрдгрддреА рдЙрддреНрдкрд╛рджрдиреЗ рдорд╛рдЧрд╡рд╛рд╡реА?"
ЁЯФЭ рд╢реАрд░реНрд╖ рдЙрддреНрдкрд╛рджрдиреЗ - рд╡рд┐рдЪрд╛рд░рд╛ "рдорд╛рдЭреА рд╕рд░реНрд╡рд╛рдзрд┐рдХ рд╡рд┐рдХреНрд░реА рд╣реЛрдгрд╛рд░реА рдЙрддреНрдкрд╛рджрдиреЗ рдХреЛрдгрддреА рдЖрд╣реЗрдд?"
тЪая╕П рдЕрд▓рд░реНрдЯ - рд╡рд┐рдЪрд╛рд░рд╛ "рдорд╛рдЧрдгреАрдд рдХрд╛рд╣реА рд╡рд╛рдв рдЖрд╣реЗ рдХрд╛?"
ЁЯУК рдЕрдВрджрд╛рдЬ - рд╡рд┐рдЪрд╛рд░рд╛ "рдкреБрдвреАрд▓ рдЖрдард╡рдбреНрдпрд╛рдЪрд╛ рдЕрдВрджрд╛рдЬ рдХрд╛рдп рдЖрд╣реЗ?"

рддреБрдордЪреНрдпрд╛рдХрдбреЗ {len(products)} рдЙрддреНрдкрд╛рджрдиреЗ рд╡рд┐рд╢реНрд▓реЗрд╖рд┐рдд рдЖрд╣реЗрдд рдЬреНрдпрд╛рдд {len(high_urgency)} рдЙрдЪреНрдЪ рдкреНрд░рд╛рдзрд╛рдиреНрдп рдкреБрдиреНрд╣рд╛ рдСрд░реНрдбрд░ рдЖрдгрд┐ {len(anomalies)} рдЕрд▓рд░реНрдЯ рдЖрд╣реЗрддред"""


def get_no_data_response(language: str) -> str:
    """Response when no insights data is available"""
    if language == 'en':
        return "ЁЯУК To get personalized recommendations, please upload your sales data first. I'll analyze demand patterns and provide actionable insights."
    elif language == 'hi':
        return "ЁЯУК рд╡реНрдпрдХреНрддрд┐рдЧрдд рд╕рд┐рдлрд╛рд░рд┐рд╢реЗрдВ рдкреНрд░рд╛рдкреНрдд рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП, рдХреГрдкрдпрд╛ рдкрд╣рд▓реЗ рдЕрдкрдирд╛ рдмрд┐рдХреНрд░реА рдбреЗрдЯрд╛ рдЕрдкрд▓реЛрдб рдХрд░реЗрдВред рдореИрдВ рдорд╛рдВрдЧ рдкреИрдЯрд░реНрди рдХрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░реВрдВрдЧрд╛ рдФрд░ рдХрд╛рд░реНрд░рд╡рд╛рдИ рдпреЛрдЧреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯрд┐ рдкреНрд░рджрд╛рди рдХрд░реВрдВрдЧрд╛ред"
    else:  # Marathi
        return "ЁЯУК рд╡реИрдпрдХреНрддрд┐рдХ рд╢рд┐рдлрд╛рд░рд╕реА рдорд┐рд│рд╡рд┐рдгреНрдпрд╛рд╕рд╛рдареА, рдХреГрдкрдпрд╛ рдкреНрд░рдердо рддреБрдордЪрд╛ рд╡рд┐рдХреНрд░реА рдбреЗрдЯрд╛ рдЕрдкрд▓реЛрдб рдХрд░рд╛. рдореА рдорд╛рдЧрдгреА рдкреЕрдЯрд░реНрдирдЪреЗ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░реЗрди рдЖрдгрд┐ рдХрд╛рд░реНрдпрд╡рд╛рд╣реА рдХрд░рдгреНрдпрд╛рдпреЛрдЧреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА рдкреНрд░рджрд╛рди рдХрд░реЗрди."
